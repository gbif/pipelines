### run options:

run:
  # where to run: local, spark-embedded or spark-cluster
  platform: local
  local:
    # jar: we get the jar from our dev or production environment
    sparkTmp: /data/spark-tmp
    sparkMaster: ""
  spark-embedded:
    # jar: we get the jar from our dev or production environment
    sparkTmp: /data/spark-tmp
    sparkMaster: ""
  spark-cluster:
    jar: /efs-mount-point/pipelines.jar
    sparkTmp: /data/spark-tmp
    sparkMaster: spark://aws-spark-quoll-1.ala:7077

# which filesystem to use: local or hdfs
fs:
  platform: local
  local:
    fsPath: /data
alaNameMatch:
  wsUrl: http://localhost:ALA_NAME_MATCH_PORT
  timeoutSec: 70
collectory:
  wsUrl: http://localhost:COLLECTORY_PORT
  timeoutSec: 70
speciesListService:
  wsUrl: http://localhost:LISTS_PORT
  timeoutSec: 70
samplingService:
  wsUrl: http://localhost:SPATIAL_PORT
#  wsUrl: https://sampling.ala.org.au/sampling-service/
  timeoutSec: 70
sds:
  wsUrl: http://localhost:SDS_PORT
  timeoutSec: 70
geocodeConfig:
  country:
    path: /tmp/pipelines-shp/political
    field: ISO_A2
    intersectBuffer: 0.135
    intersectMapping:
      CX: AU
      CC: AU
      HM: AU
      NF: AU
  eez:
    path: /tmp/pipelines-shp/eez
    field: ISO2
    intersectBuffer:  0.135
  stateProvince:
    path: /tmp/pipelines-shp/cw_state_poly
    field: FEATURE
    intersectBuffer: 0.135
  biome:
    path: /tmp/pipelines-shp/gadm0
    field: FEATURE
gbifConfig:
  extensionsAllowedForVerbatimSet:
    - http://rs.tdwg.org/ac/terms/Multimedia
    - http://data.ggbn.org/schemas/ggbn/terms/Amplification
    - http://data.ggbn.org/schemas/ggbn/terms/Cloning
    - http://data.ggbn.org/schemas/ggbn/terms/GelImage
    - http://data.ggbn.org/schemas/ggbn/terms/Loan
    - http://data.ggbn.org/schemas/ggbn/terms/MaterialSample
    - http://data.ggbn.org/schemas/ggbn/terms/Permit
    - http://data.ggbn.org/schemas/ggbn/terms/Preparation
    - http://data.ggbn.org/schemas/ggbn/terms/Preservation
    - http://rs.iobis.org/obis/terms/ExtendedMeasurementOrFact
    - http://rs.tdwg.org/chrono/terms/ChronometricAge
    - http://purl.org/germplasm/germplasmTerm#GermplasmAccession
    - http://purl.org/germplasm/germplasmTerm#MeasurementScore
    - http://purl.org/germplasm/germplasmTerm#MeasurementTrait
    - http://purl.org/germplasm/germplasmTerm#MeasurementTrial
    - http://rs.tdwg.org/dwc/terms/Identification
    - http://rs.tdwg.org/dwc/terms/Occurrence
    - http://rs.gbif.org/terms/1.0/Identifier
    - http://rs.gbif.org/terms/1.0/Image
    - http://rs.tdwg.org/dwc/terms/MeasurementOrFact
    - http://rs.gbif.org/terms/1.0/Multimedia
    - http://rs.gbif.org/terms/1.0/Reference
    - http://rs.tdwg.org/dwc/terms/ResourceRelationship
    - http://rs.gbif.org/terms/1.0/DNADerivedData
  vocabularyConfig:
    vocabulariesPath: src/test/resources/vocabularies/
    vocabulariesNames:
      http://rs.tdwg.org/dwc/terms/degreeOfEstablishment: DegreeOfEstablishment
      http://rs.tdwg.org/dwc/terms/lifeStage: LifeStage
      http://rs.tdwg.org/dwc/terms/establishmentMeans: EstablishmentMeans
      http://rs.tdwg.org/dwc/terms/pathway: Pathway
      http://rs.gbif.org/terms/1.0/eventType: EventType
      http://rs.tdwg.org/dwc/terms/occuurrenceStatus: OccurrenceStatus

pipelineExcludeArgs: fsPath

# Common PipelineOptions
general:
  # Target path where the outputs of the pipeline will be written to
  targetPath: '{fsPath}/pipelines-data'
  # Attempt of the dataset used to name the target file in file system
  attempt: 1
  # The absolute path to a hdfs-site.xml with default.FS configuration
  hdfsSiteConfig: ""
  # Path to core-site-config.xml
  coreSiteConfig: ""

# class: au.org.ala.pipelines.beam.ALADwcaToVerbatimPipeline
dwca-avro:
  runner: SparkRunner
  metaFileName: dwca-metrics.yml

# class: au.org.ala.pipelines.beam.ALAVerbatimToInterpretedPipeline
interpret:
  appName: Interpretation for {datasetId}
  interpretationTypes: ALL
  inputPath: '{fsPath}/pipelines-data/{datasetId}/1/verbatim/*.avro'
  metaFileName: interpretation-metrics.yml
  useExtendedRecordId: true
  runner: SparkRunner
  name: interpret {datasetId}

# class: au.org.ala.pipelines.beam.ALAUUIDMintingPipeline
uuid:
  appName: UUID minting for {datasetId}
  runner: SparkRunner
  inputPath: '{fsPath}/pipelines-data'
  metaFileName: uuid-metrics.yml

# class: au.org.ala.pipelines.beam.ALAInterpretedToLatLongCSVPipeline
sampling:
  appName: Sampling for {datasetId}
  inputPath: '{fsPath}/pipelines-data'
  runner: SparkRunner

speciesLists:
  runner: SparkRunner
  speciesAggregatesPath: 'tmp./pipelines-species'
  tempLocation: /tmp
  maxDownloadAgeInMinutes: 1440
  includeConservationStatus: true
  includeInvasiveStatus: true

# class: au.org.ala.pipelines.beam.ALASamplingToAvroPipeline
sample-avro:
  inputPath: '{fsPath}/pipelines-data'
  runner: SparkRunner
  metaFileName: sampling-metrics.yml

# class: au.org.ala.pipelines.java.ALAInterpretedToSolrIndexPipeline
index:
  inputPath: '{fsPath}/pipelines-data'
  metaFileName: indexing-metrics.yml

solr:
  inputPath: '{fsPath}/pipelines-data'
  metaFileName: solr-metrics.yml
  solrCollection: biocache
  includeSampling: true
  zkHost: localhost:SOLR_PORT
  numOfPartitions: 10

elastic:
  appName: Elastic indexing for {datasetId}
  inputPath: '{fsPath}/pipelines-data'
  targetPath: '{fsPath}/pipelines-data'
  esHosts: http://localhost:ES_PORT
  esAlias: event
  esIndexName: 'event_{datasetId}'
  indexNumberShards: 1
  indexNumberReplicas: 0
  esDocumentId: internalId

predicate-export:
  inputPath: '{fsPath}/pipelines-data'
  targetPath: '/tmp/pipelines-export-prepare'
  skipEventExportFields:
    - "id"
    - "parentId"
    - "created"
    - "hasGeospatialIssue"
    - "hasCoordinate"
    - "repatriated"
    - "coreId"
  skipOccurrenceExportFields:
    - "id"
    - "parentId"
    - "created"
    - "hasGeospatialIssue"
    - "hasCoordinate"
    - "repatriated"
    - "coreId"

search-avro:
  appName: Search AVRO for {datasetId}
  inputPath: '{fsPath}/pipelines-data'
  targetPath: '{fsPath}/pipelines-data'

# class: au.org.ala.utils.DumpDatasetSize
dataset-count-dump:
  inputPath: '{fsPath}/pipelines-data'
  targetPath: /tmp/dataset-counts.csv

migrate-uuids:
  inputPath: '{fsPath}/pipelines-data/occ_uuid.csv'
  targetPath: '{fsPath}/pipelines-data'
  hdfsSiteConfig: ""

### la-pipelines cli additional arguments, like JVM or spark command line arguments
interpret-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 16
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 1G

uuid-sh-args:
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    num-executors: 24
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 1G

export-sampling-sh-args:
  spark-embedded:
    jvm:
  spark-cluster:
    num-executors: 8
    executor-cores: 8
    executor-memory: 16G
    driver-memory: 4G

sample-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC

sample-avro-sh-args:
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=192
    num-executors: 24
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 1G

index-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=192
    num-executors: 24
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 4G
test:
  zkHost: localhost:9983
  solrAdminHost: localhost:8983

defaultDateFormat:
  - DMYT
  - DMY

root-test: 1

unicode-test: Лорем ипсум долор сит амет, дуо еа прима семпер