# which filesystem to use: local or hdfs
run:
  # where to run: local, spark-embedded or spark-cluster
  platform: local
  local:
    # jar: we get the jar from our dev or production environment
    sparkTmp: /data/spark-tmp
    sparkMaster: "DONT_SET"
    dwcaTmp: /data/dwca-tmp
    dwcaImportDir: /data/dwca-imports
  spark-embedded:
    dwcaTmp: /data/dwca-tmp
    dwcaImportDir: /data/dwca-imports
    sparkTmp: /data/spark-tmp
    sparkMaster: "DONT_SET"
  spark-cluster:
    dwcaTmp: /data/dwca-tmp
    dwcaImportDir: /data/dwca-imports
    jar: /usr/share/la-pipelines/la-pipelines.jar
    sparkSubmit: /usr/bin/spark-submit
    sparkTmp: /data/spark-tmp
    # dont specify spark master for EMR
    sparkMaster: "DONT_SET"

general:
  attempt: 1
  hdfsSiteConfig: /etc/hadoop/conf/hdfs-site.xml
  coreSiteConfig: /etc/hadoop/conf/core-site.xml

fs:
  platform: local
  local:
    fsPath: /data
  hdfs:
    fsPath: hdfs:///

dataset-validated-dump:
  inputPath: hdfs:///pipelines-data
dataset-count-dump:
  inputPath: hdfs:///pipelines-data
dataset-archive-list:
  inputPath: hdfs:///dwca-exports/

dwca-avro:
  inputPath: /data/biocache-load/{datasetId}
  tempLocation: /data/biocache-load/{datasetId}
  targetPath: hdfs:///pipelines-data

interpret:
  appName: Interpretation for {datasetId}
  inputPath: hdfs:///pipelines-data/{datasetId}/1/verbatim/*.avro
  targetPath: hdfs:///pipelines-data/
  defaultDateFormat: DMYT,DMY

uuid:
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-data
  throwErrorOnValidationFail: false

sensitive:
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-data

images:
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-data

speciesLists:
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-data
  speciesAggregatesPath: hdfs:///pipelines-species

index:
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-data
  allDatasetsInputPath: hdfs:///pipelines-all-datasets

solr:
  numOfPartitions: 10
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-data
  allDatasetsInputPath: hdfs:///pipelines-all-datasets
  jackKnifePath: hdfs:///pipelines-jackknife
  clusteringPath: hdfs:///pipelines-clustering
  outlierPath: hdfs:///pipelines-outlier
  zkHost: ZK_URL
  solrCollection: SOLR_COLLECTION

jackKnife:
  jackKnifePath: hdfs:///pipelines-jackknife
  allDatasetsInputPath: hdfs:///pipelines-all-datasets

clustering:
  clusteringPath: hdfs:///pipelines-clustering
  allDatasetsInputPath: hdfs:///pipelines-all-datasets

sampling:
  inputPath: hdfs:///pipelines-data
  allDatasetsInputPath: hdfs:///pipelines-all-datasets

outlier:
  baseUrl: SAMPLING_URL
  targetPath: hdfs:///pipelines-outlier
  allDatasetsInputPath: hdfs:////pipelines-all-datasets

elastic:
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-data
  esSchemaPath: /tmp/es-event-core-schema.json
  esHosts: ES_HOSTS
  esAlias: ES_ALIAS
  indexNumberShards: 6

export:
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-data

predicate-export:
  inputPath: hdfs:///pipelines-data
  targetPath: hdfs:///pipelines-export

gbifConfig:
  vocabularyConfig:
    vocabulariesPath: /data/pipelines-vocabularies/
    vocabulariesNames:
      http://rs.tdwg.org/dwc/terms/degreeOfEstablishment: DegreeOfEstablishment
      http://rs.tdwg.org/dwc/terms/lifeStage: LifeStage
      http://rs.tdwg.org/dwc/terms/establishmentMeans: EstablishmentMeans
      http://rs.tdwg.org/dwc/terms/pathway: Pathway
      http://rs.gbif.org/terms/1.0/eventType: EventType
      http://rs.tdwg.org/dwc/terms/occurrenceStatus: OccurrenceStatus

collectory:
  wsUrl: REGISTRY_URL
  httpHeaders:
    Authorization: ALA_API_KEY

alaNameMatch:
  wsUrl: NAME_MATCHING_URL

sds:
  wsUrl: SDS_URL

imageService:
  wsUrl: IMAGES_URL
  httpHeaders:
    apiKey: ALA_API_KEY

speciesListService:
  wsUrl: LISTS_URL

samplingService:
  wsUrl: SAMPLING_URL


### la-pipelines cli additional arguments, like JVM or spark command line arguments
interpret-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 6G

image-sync-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

image-load-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

uuid-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

sampling-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

outlier-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

sensitive-sh-args:
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    jvm: -Xmx8g -XX:+UseG1GC
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

sample-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC

index-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

jackknife-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

clustering-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

export-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

elastic-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G

solr-sh-args:
  local:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx8g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=500
    num-executors: 8
    executor-cores: 8
    executor-memory: 18G
    driver-memory: 2G
